#import "template.typ" as gost

#set document(title: "КР Орлова, Ершова", author: "Орлова С. Н., Ершова О. А.")

#show: gost.template

#outline()

#gost.struct-heading("ВВЕДЕНИЕ")

В современной индустрии разработки программного обеспечения ключевую роль играют практики DevOps, направленные на автоматизацию и ускорение жизненного цикла продукта. Одними из фундаментальных инструментов в этой методологии являются системы непрерывной интеграции и непрерывной доставки (Continuous Integration (CI), Continuous Delivery (CD)), которые позволяют автоматически собирать исходное приложение, тестировать его, разворачивать приложение на целевой среде после каждого изменения.

Несмотря на наличие коммерческих и open-source решений, таких как GitLab CI или GitHub Actions, их настройка и использование в небольших проектах или в образовательных целях может быть избыточно сложной. Это создает потребность в разработке легковесных, простых для понимания и развертывания CI/CD-систем, которые реализуют ключевые принципы автоматизации, но не требуют значительных ресурсов для поддержки.

Данная курсовая работа посвящена проектированию и разработке системы "Swompi-Runner". Это CI/CD-сервер, который отслеживает изменения в Git-репозиториях, автоматически запускает пользовательские сценарии в Docker-контейнерах и уведомляет о результатах через Telegram. Система полностью управляется по принципу "Configuration as Code" с помощью YAML-файла, что соответствует современным стандартам в области DevOps.

Цель работы: приобретение профессиональных умений и
навыков, опыта самостоятельной профессиональной деятельности и закрепление
приобретенных компетенций.

Для достижения поставленной цели были определены следующие задачи:
+ Рассмотреть предметную область систем непрерывной интеграции, проанализировать существующие аналоги и на их основе спроектировать архитектуру и модульную структуру собственного приложения.
+ Выбрать и обосновать стек технологий.
+ Разработать структуру базы данных для хранения информации об отслеживаемых репозиториях, истории сборок и их статусах.
+ Реализовать серверную часть приложения, отвечающую за прием веб-хуков от GitHub и запуск исполнителя (Executor).
+ Реализовать модуль-исполнитель Executor, управляющий жизненным циклом Docker-контейнеров, выполнением пользовательских скриптов и сбором артефактов.
+ Реализовать клиентскую часть в виде Telegram-бота и CLI-утилиты для администратора, предоставляющих интерфейс для взаимодействия с системой.
+ Разработать тестовое окружение с использованием Docker Compose для удобного развертывания всех компонентов системы (приложение, база данных, файловое хранилище).
+ Развернуть систему на удаленном VPS-сервере, протестировать ее работу в реальных условиях.

Работа выполнялась командой из двух человек. Распределение задач было произведено следующим образом:
- Орлова С.Н.: Проектирование и реализация ядра системы (модуль-исполнитель Executor), настройка Docker Compose, разработка CLI, взаимодействие с файловым хранилищем.
- Ершова О.А.: структура БД, модели SQLAlchemy, функции для работы с данными, разработка Telegram-бота.

#pagebreak(weak: true)

= Анализ предметной области и существующих решений

Для проектирования системы непрерывной интеграции "Swompi-Runner" в первую очередь был проведен анализ предметной области. Были изучены основные принципы и цели CI/CD, рассмотрены ключевые существующие решения на рынке, а также сформулировано обоснование для разработки собственного программного продукта в рамках курсовой работы.

== Описание систем непрерывной интеграции (CI)

Непрерывная интеграция (Continuous Integration, CI) представляет собой практику разработки программного обеспечения, при которой разработчики регулярно вносят изменения в код основного репозитория.

Основной целью практики CI является повышение качества программного продукта и ускорение цикла разработки за счет автоматизации процессов сборки и тестирования. Ключевым событием, запускающим процесс, как правило, являлась отправка (push) нового кода в систему контроля версий.

После получения уведомления о событии CI-сервер выполнял последовательность заранее определенных шагов, именуемую пайплайн (pipeline). Стандартный конвейер включает в себя следующие этапы:
1.  Клонирование исходного кода с системы контроля версий.
2.  Установка необходимых зависимостей, библиотек и инструментов.
3.  Статический анализ кода.
4.  Автоматический запуск тестов.
5.  Сохранение исполняемых файлов, отчетов, Docker-образов.
6.  По результатам выполнения пайплайна разработчикам отправляется отчет о статусе сборки с доступом к логам выполнения.

== Сравнительный анализ аналогов

Для проведения сравнительного анализа были выбраны три ключевые системы, каждая из которых различный архитектурный подход в области непрерывной интеграции.

В качестве первого аналога был выбран Jenkins. Он представляет классический подход с архитектурой "master-agent, сильной расширяемостью за счет системы плагинов и конфигурации пайплайна через код. 

Вторым объектом для анализа стала система GitLab CI. Она была выбрана как представитель подхода интеграции CI/CD-инструментов в единую DevOps-платформу. GitLab CI не является отдельным продуктом, а представляет собой неотъемлемую часть GitLab, что обеспечивает взаимодействие с репозиториями, реестром контейнеров и другими сервисами платформы.

В качестве третьего аналога была выбрана платформа GitHub Actions. Его ключевой особенностью является концепция переиспользуемых "действий" (actions), которые могут быть созданы сообществом и легко встроены в любой пайплайн, что значительно ускоряет процесс его создания.

== Проведение анализа

Для обеспечения объективности и структурированности анализа был определен набор из пяти ключевых критериев. Данные критерии были выбраны с целью охватить как технические аспекты реализации и архитектуры, так и аспекты, связанные с пользовательским опытом и процессом эксплуатации систем.

+ Критерий "Модель конфигурации" определяет способ описания пайплайна. Анализ проводился на предмет того, используется ли декларативный подход (описание конечного состояния в формате YAML), императивный (написание скрипта с последовательностью действий, например, на Groovy) или конфигурация через графический пользовательский интерфейс (UI).

+ Критерий "Архитектура и модель развертывания" описывает внутреннее устройство системы и способы ее установки. Рассматривались такие архитектурные модели, как "master-agent" или интегрированная в платформу, а также модели развертывания: полностью самоуправляемая (self-hosted) или предоставляемая как облачный сервис (SaaS).

+ Среда выполнения задач. Сравнивалось окружение, в котором выполняются пользовательские команды. Сравнение проводилось по тому, используются ли для изоляции сборок Docker-контейнеры, полноценные виртуальные машины или выполнение происходит в операционной системе сервера-исполнителя ("агента").

+ Критерий "Уровень интеграции с Git-провайдером" оценивает, насколько тесно CI-система связана с конкретной платформой для хостинга репозиториев. Системы были разделены на универсальные, способные работать с любым Git-репозиторием, и платформо-зависимые, функционирующие только в рамках экосистемы.

+ Критерий "Порог вхождения и сложность настройки" отражает совокупность усилий, необходимых для первоначального развертывания, базовой настройки и последующей поддержки системы. Оценивалась как сложность написания пайплайна, так и сложность администрирования системы.

Был выполнен последовательный анализ каждой из трех выбранных систем в соответствии с ранее определенными критериями.

+ *Jenkins*

  Jenkins представляет собой сервер автоматизации с открытым исходным кодом, написанный на Java. Он являлся одним из первых инструментов в своей области, благодаря чему получил широкое распространение и огромное сообщество.

  Jenkins поддерживает два подхода к конфигурации. Изначально создание и настройка задач происходила через веб-интерфейс. Однако сейчас все чаще используется описание конфигурации в файле Jenkinsfile с использованием императивного или декларативного синтаксиса на языке Groovy.

  Система построена на классической архитектуре "master-agent". Центральный узел (master) отвечает за хранение конфигураций, управление задачами и распределение нагрузки. Выполнение сборок осуществляется на подключенных к нему узлах-исполнителях (agents). Jenkins является self-hosted решением, то есть он настраивается и располагается полностью на физических устройствах пользователя.

  Задачи могут выполняться как в операционной системе узла-агента, так и в изолированных Docker-контейнерах.

  Система является полностью универсальной и не зависит от конкретного Git-провайдера. Интеграция с любыми сервисами, такими как GitHub, GitLab или Bitbucket, реализуется посредством установки и настройки соответствующих плагинов.

  Порог вхождения для работы с Jenkins оценивается как высокий. Первоначальное развертывание и настройка требуют значительных усилий, включая установку Java, управление самим сервером и конфигурацию узлов-агентов. Управление большим количеством плагинов также приводит к сложностям с зависимостями и обновлениями.

+ *GitLab CI*

  GitLab CI представляет собой компонент, интегрированный в DevOps-платформу GitLab.

  Для описания пайплайнов используется декларативный подход. Вся конфигурация задается в едином файле .gitlab-ci.yml, который располагается в корне репозитория и использует стандартный синтаксис YAML.

  Архитектура связана с платформой GitLab, которая выполняет роль управляющего узла. За выполнение задач отвечают агенты --- GitLab Runners, которые разворачиваются на серверах. GitLab CI доступен как в облачной версии (SaaS) на GitLab.com, так и как система, разворачиваемая и используемая на физических устройствах пользователя.

  Основной средой выполнения являются Docker-контейнеры. Также поддерживаются и другие среды, например, для запуска команд в Kubernetes или в оболочке сервера.

  Система полностью интегрирована и предназначена для работы с репозиториями, размещенными на платформе GitLab.

  Для пользователей, уже работающих с GitLab, порог вхождения является средним, так как CI-компонент интегрирован в систему. Процесс написания .gitlab-ci.yml интуитивно понятен и хорошо описан в документации. Установка и настройка собственного GitLab Runner требует определенных начальных усилий, однако это не является обязательным действием, так как Gitlab предоставляет бесплатных агентов.

+ *GitHub Actions*

  Инструмент для автоматизации, встроенный в платформу GitHub.

  Используется декларативная модель на основе YAML-файлов, которые должны располагаться в директории .github/workflows/. Платформа позволяет создавать сложные пайплайны с параллельным выполнением и зависимостями между задачами.

  Система интегрирована в GitHub, который выступает в роли оркестратора. Выполнение задач происходит на исполнителях (runners). GitHub предоставляет собственные облачные исполнители с различными операционными системами (Linux, Windows, macOS), а также позволяет подключать self-hosted исполнители.

  В облачной версии задачи выполняются на изолированных виртуальных машинах. Ключевой концепцией является "действие" (action), которое часто представляет собой Docker-контейнер или набор JavaScript-команд, выполняющих определенную логику.

  GitHub Actions предназначен для работы только с GitHub репозиториями и тесно интегрирован с экосистемой событий Github (push, pull request, release и т.д.).

  Порог вхождения оценивается как низкий. Начать работу можно за несколько минут благодаря обширному магазину готовых "действий" и качественной интеграции с интерфейсом GitHub. Настройка самоуправляемых исполнителей проста и задокументированна.

== Сводная сравнительная таблица

Для наглядного представления результатов проведенного анализа все данные были сведены в @fig-ci-comparison-table[таблицу]. В таблицу также был включен разрабатываемый проект "Swompi-Runner" для демонстрации его позиционирования относительно рассмотренных аналогов.

#figure(
  table(
    columns: (1.5fr, 1fr, 1fr, 1fr, 1fr),
    align: (left, center, center, center, center),
    [*Критерий*], [*Jenkins*], [*GitLab CI*], [*GitHub Actions*], [*Swompi-Runner*],

    [Модель конфигурации],
    [Groovy, UI],
    [YAML],
    [YAML],
    [YAML],

    [Архитектура и развертывание],
    [Master-Agent, Self-hosted],
    [Интегрировано, Self-hosted / SaaS],
    [Интегрировано, SaaS],
    [Единый сервис, Self-hosted],

    [Среда выполнения задач],
    [ОС агента, Docker],
    [Docker, VM],
    [VM, Docker],
    [Docker],
    
    [Интеграция с Git],
    [Любой Git-провайдер],
    [Только GitLab],
    [Только GitHub],
    [Любой Git-провайдер],

    [Порог вхождения],
    [Высокий],
    [Средний],
    [Низкий],
    [Низкий],
  ),
  caption: "Сравнительные характеристики систем непрерывной интеграции."
) <fig-ci-comparison-table>

== Обоснование необходимости разработки собственного решения

Несмотря на широкую функциональность проанализированных систем, был выявлен ряд факторов, которые обосновали целесообразность разработки собственного решения в рамках курсовой работы.

Во-первых, промышленные решения обладали избыточной сложностью для небольших проектов и требовали значительных ресурсов для развертывания и поддержки. Интегрированные платформы, в свою очередь, привязывали разработчика к конкретному хостингу репозиториев (GitHub или GitLab).

Во-вторых, образовательная цель проекта заключалась в изучении механизмов работы CI-систем. Создание собственного сервера позволило на практике исследовать такие процессы, как обработка веб-хуков, управление Docker-контейнерами через API, захват и потоковая передача логов, а также проектирование базы данных и хранилища файлов.

Таким образом, система "Swompi-Runner" должна быть легковесной, простой в развертывании и поддержании инфраструктуры.

#pagebreak(weak: true)

= Выбор стека технологий

При выборе стека технологий для каждого компонента системы были рассмотрены альтернативы, и итоговое решение принималось на основе требований к надежности и простоте эксплуатации.

+ Язык программирования

  В качестве основного языка программирования был выбран Python 3.13. Рассматривался язык Go, который часто применяется для создания DevOps-инструментов из-за своей высокой производительности и компиляции в бинарный файл. Однако для данного проекта скорость разработки и обширность библиотек были признаны более приоритетными. Python позволил  ускорить процесс реализации за счет наличия готовых библиотек для решения всех поставленных задач: от создания веб-сервера до взаимодействия с Docker API и S3-хранилищем.

+ Серверная часть

  Для реализации компонента, ответственного за прием веб-хуков от GitHub, рассматривались два популярных фреймворка FastAPI и Flask. Задачей сервера являлась реализация всего одного API-эндпоинта, поэтому функциональность FastAPI была признана избыточной. В связи с этим был сделан выбор в пользу микрофреймворка Flask.

+ База данных и файловое хранилище

  На этапе проектирования было принято архитектурное решение о разделении данных на два типа: структурированные метаданные (информация о сборках и репозиториях) и неструктурированные объектные данные (логи и артефакты).

  Для хранения структурированных данных рассматривались СУБД SQLite и PostgreSQL. SQLite была отклонена из-за ограничений в работе с параллельными операциями записи. Выбор был сделан в пользу PostgreSQL как полнофункциональной, надежной и производительной СУБД, легко настраеваемой через Docker-контейнер. Для взаимодействия с ней была выбрана ORM SQLAlchemy, которая позволила работать с данными в объектно-ориентированном стиле. В качестве драйвера был использован psycopg2-binary.

  Для хранения логов и артефактов рассматривался вариант использования локальной файловой системы, однако он был отклонен из-за сложностей с масштабированием и управлением. Далее выбор пал на MinIO, но из-за недавнего изменения политики и закрытия разработки open-source версии, что значит технология не будет развиваться дальше, было выбрано S3-совместимое объектное хранилище Garage.

+ Контейнеризация и оркестрация

  Docker был выбран как отраслевой стандарт, позволяющий запускать пользовательские сценарии в полностью изолированном окружении. Для программного управления контейнерами из Python-кода была использована библиотека docker. Для объединения контейнеров в единую систему и их последующее администрирование был выбран инструмент Docker Compose.

+ Конфигурация приложения и парсинг

  Для управления конфигурацией самого приложения была выбрана библиотека Pydantic-settings за ее способность автоматически загружать параметры из переменных окружения с одновременной валидацией типов. Для парсинга пользовательских файлов .swompi.yml была использована связка из PyYAML для чтения YAML-структуры и Schema для валидации. Для взаимодействия с Git-репозиториями была выбрана библиотека GitPython, так как она предоставляет более надежный и объектно-ориентированный интерфейс по сравнению с прямым вызовом системных команд git.

+ Реализация клиентских интерфейсов

  Для создания утилиты командной строки рассматривались стандартный модуль argparse и библиотека Click. Выбор был сделан в пользу Click из-за его современного подхода с использованием декораторов, что делает код более чистым и читаемым.


#pagebreak(weak: true)


= Архитектура приложения

+ *Обработчик веб-хуков (файл main.py) *

  Данный модуль является основной точкой входа в систему. Он был реализован как веб-сервер на базе Flask, прослушивающий POST-запросы на порту 25851. Модуль проверяет, добавлена ли ссылка на репозиторий в базу данных, если да, то извлекает из тела запроса информацию (URL репозитория, хэш коммита, данные об авторе), создает предварительную запись о новой сборке в базе данных со статусом "pending", запускает модуль-исполнитель и отправляет ответ со статусом 200. Если же URL репозитория не добавлен в систему "Swompi-Runner", то запрос отклоняется со статусом 400. Запуск веб-сервера показан на @image2[рисунке].

  #figure(image("img/image copy.png"), caption: [Запуск веб-сервера]) <image2>

+ *Исполнитель (файл executor.py) *

  Реализован класс Executor, который при инициализации запрашивает фабрику создания сессий к базе данных, объект клиента файлового хранилища, объект Config для доступа к переменным окружения. 
  Функции класса: 
  - Основной управляющий метод run_build. Данный метод является основной точкой входа в класс и управляет всем жизненным циклом выполнения пайплайна. Его работа  обернута в конструкцию try...except...finally для гарантированной обработки ошибок и очистки ресурсов. В рамках его выполнения последовательно вызывались остальные приватные методы.

  - Подготовка временной директории (\_prepare_workspace). Для обеспечения полной изоляции каждой сборки была реализована функция, которая с использованием стандартной библиотеки tempfile создавала уникальную временную директорию в системной папке /tmp/. Для удобства отладки каждой директории был присвоен префикс swompi_build_\{build_id\}_.

  - Клонирование репозитория (\_clone_repo). На следующем этапе была реализована логика клонирования Git-репозитория. С помощью библиотеки GitPython выполнялась команда clone_from, загружавшая исходный код, и выполнялся переход (checkout) к конкретному коммиту, инициировавшему запуск пайплайна.

  - Чтение и валидация конфигурационного файла (\_read_and_validate_config). Сначала вспомогательный метод \_find_config_file находил файл .swompi.yml в корне репозитория. Затем основной метод \_read_and_validate_config валидировал его содержимое. С использованием библиотеки Schema была определена строгая структура ожидаемого файла, а чтение и парсинг YAML-структуры осуществлялись с помощью PyYAML.

  - Формирование переменных окружения (\_create_enviroment_dict). Был реализован метод, который формировал словарь переменных окружения для передачи в Docker-контейнер. Этот словарь включал как предопределенные системой переменные (CI_COMMIT_SHA, CI_COMMIT_MESSAGE, CI_COMMIT_AUTHOR, CI_PROJECT_DIR, CI_REPO_URL, CI_BUILD_ID, CI_SERVER_NAME, CI_COMMIT_REF_NAME), так и пользовательские, указанные в файле .swompi.yml.

  - Создание исполняемого скрипта (\_create_build_script). Был применен подход с созданием временного shell-скрипта \_run.sh. В этот скрипт последовательно записывались команды из секций before_script и scripts с добавлением директивы set -e, гарантирующей завершение работы при возникновении любой ошибки.

  - Управление Docker-контейнером (\_run_docker_container). С помощью библиотеки docker был реализован алгоритм, включавший загрузку Docker-образа, создание и запуск контейнера с монтированной временной директорией и переменными окружения, а также захват логов в реальном времени.

  - Обработка сбоев (\_mark_build_as_failed). В случае возникновения исключения или при получении ненулевого кода выхода от контейнера вызывался данный метод. Он обновлял статус текущей сборки в базе данных на failed.

  - Очистка рабочего пространства (\_cleanup_workspace). Был реализован метод, который вызывался в блоке finally основного метода. Он полностью и рекурсивно удалял временную директорию со всем ее содержимым.

+ *Взаимодействие с базой данных *


+ *Файловое хранилище (файл storage.py)*

  Для инкапсуляции всей логики взаимодействия с S3-совместимым файловым хранилищем Garage был разработан класс FileStorageRepository. Данный компонент отвечал за загрузку и скачивание файлов, а также за первоначальную настройку окружения в хранилище. Его функциональность была разделена на следующие методы:

  - Инициализация и подключение к S3 (\_create_s_client). При создании экземпляра класса в конструктор \__init_\_ передавался объект конфигурации приложения. Этот метод  использовал данные для инициализации клиента boto3. Для обеспечения совместимости с self-hosted решением Garage была явно указана версия подписи s3v4 и применен path-style для адресации бакетов.

  - Автоматическое создание бакета (\_ensure_buckets_exist). Метод, вызывающийся сразу после инициализации клиента, реализовывал принцип идемпотентности: он отправлял запрос head_bucket для проверки существования необходимого бакета "swompi-runner". В случае, если бакет не был найден, метод автоматически отправлял команду на его создание.

  - Загрузка логов и артефактов (upload_logs_and_artifacts). Все файлы сборки, включая лог и артефакты, объединяются в один архив. Для этого создается временная директория, в которую перемещаются файл build.log и все файлы, указанные в секции artifacts. Затем, с использованием библиотеки py7zr, содержимое директории сжимается в единый архив формата .7z, который загружается в S3-хранилище под именем, соответствующим идентификатору сборки. Метод возвращает имя созданного объекта.

  - Скачивание файлов (download_file_to_path). Для выгрузки архива метод принимает на вход ключ объекта и локальный путь для сохранения, после чего выполняется скачивание файла.

+ *Модуль конфигурации (файл config.py)*

  Для управления конфигурационными параметрами приложения, такими как учетные данные для доступа к базам данных и внешним API, был применен подход, основанный на отделении конфигурации от кода. 

  Вся конфигурация была инкапсулирована в классе AppConfig, унаследованном от класса BaseSettings. Внутри этого класса были объявлены все необходимые для работы системы параметры в виде атрибутов с явным указанием типов данных.

  Для параметров POSTGRES_HOSTNAME, S3_ENDPOINT_URL, S3_DEFAULT_REGION были заданы значения по умолчанию. Для переменных S3_ACCESS_KEY, S3_SECRET_KEY и TELEGRAM_BOT_TOKEN, значения по умолчанию отсутствовали. Приложение не может запуститься без предоставления этих параметров, что исключает ошибки во время выполнения, связанные с отсутствием необходимых учетных данных.

  Переменные POSTGRES_DB и POSTGRES_USER указываются в файле .env в корневой папке приложения "Swompi-Runner". 

+ *Интерфес командной строки (файл cli.py)*

  Для управления системой был разработан интерфейс командной строки для работы со списком отслеживаемых репозиториев. Для создания CLI была выбрана библиотека Click.

  Основная логика была сгруппирована вокруг объекта group, который соответствовал главной исполняемой команде swompi. К этой группе были привязаны три подкоманды, реализующие базовые CRUD-операции.

  - Команда ls. Была предназначена для отображения полного списка репозиториев, которые в данный момент отслеживаются системой.

  - Команда create. Позволяет добавлять новые репозитории для отслеживания. Была добавлена проверка входных данных, гарантирующая, что предоставленный URL действительно является ссылкой на сервис GitHub.

  - Команда delete. Используется для удаления репозитория из списка отслеживаемых. Метод получал URL репозитория в качестве единственного аргумента, после чего вызывал функцию базы данных, которая выполняла поиск и удаление соответствующей записи.

Структура проекта показана на @image1[рисунке]. 

#figure(image("img/image.png"), caption: [Структура проекта]) <image1>

= Docker Compose и Dockerfiles



= Структура .swompi.yml файла <structure-swompi-yml>


= Демонстрация работы


