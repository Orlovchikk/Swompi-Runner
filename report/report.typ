#import "template.typ" as gost

#set document(title: "КР Орлова, Ершова", author: "Орлова С. Н., Ершова О. А.")

#show: gost.template

#counter(page).update(2)

#outline()

#gost.struct-heading("ВВЕДЕНИЕ")

В современной индустрии разработки программного обеспечения ключевую роль играют
практики DevOps, направленные на автоматизацию и ускорение жизненного цикла
продукта. Одними из фундаментальных инструментов в этой методологии являются
системы непрерывной интеграции и непрерывной доставки (Continuous Integration
(CI), Continuous Delivery (CD)), которые позволяют автоматически собирать
исходное приложение, тестировать его, разворачивать приложение на целевой среде
после каждого изменения.

Несмотря на наличие коммерческих и open-source решений, таких как GitLab CI или
GitHub Actions, их настройка и использование в небольших проектах или в
образовательных целях может быть избыточно сложной. Это создает потребность в
разработке легковесных, простых для понимания и развертывания CI/CD-систем,
которые реализуют ключевые принципы автоматизации, но не требуют значительных
ресурсов для поддержки.

Данная курсовая работа посвящена проектированию и разработке системы "Swompi-Runner".
Это CI/CD-сервер, который отслеживает изменения в Git-репозиториях,
автоматически запускает пользовательские сценарии в Docker-контейнерах и
уведомляет о результатах через Telegram. Система полностью управляется по
принципу "Configuration as Code" с помощью YAML-файла, что соответствует
современным стандартам в области DevOps.

Цель работы: приобретение профессиональных умений и навыков, опыта
самостоятельной профессиональной деятельности и закрепление приобретенных
компетенций.

Для достижения поставленной цели были определены следующие задачи:
+ Рассмотреть предметную область систем непрерывной интеграции, проанализировать
  существующие аналоги и на их основе спроектировать архитектуру и модульную
  структуру собственного приложения.
+ Выбрать и обосновать стек технологий.
+ Разработать структуру базы данных для хранения информации об отслеживаемых
  репозиториях, истории сборок и их статусах.
+ Реализовать серверную часть приложения, отвечающую за прием веб-хуков от GitHub
  и запуск исполнителя (Executor).
+ Реализовать модуль-исполнитель Executor, управляющий жизненным циклом
  Docker-контейнеров, выполнением пользовательских скриптов и сбором артефактов.
+ Реализовать клиентскую часть в виде Telegram-бота и CLI-утилиты для
  администратора, предоставляющих интерфейс для взаимодействия с системой.
+ Разработать тестовое окружение с использованием Docker Compose для удобного
  развертывания всех компонентов системы (приложение, база данных, файловое
  хранилище).
+ Развернуть систему на удаленном VPS-сервере, протестировать ее работу в реальных
  условиях.

Работа выполнялась командой из двух человек. Распределение задач было
произведено следующим образом:
- Орлова С.Н.: Проектирование и реализация ядра системы (модуль-исполнитель
  Executor), настройка Docker Compose, разработка CLI, взаимодействие с файловым
  хранилищем.
- Ершова О.А.: структура БД, модели SQLAlchemy, функции для работы с данными,
  разработка Telegram-бота.

#pagebreak(weak: true)

= Анализ предметной области и существующих решений

Для проектирования системы непрерывной интеграции "Swompi-Runner" в первую
очередь был проведен анализ предметной области. Были изучены основные принципы и
цели CI/CD, рассмотрены ключевые существующие решения на рынке, а также
сформулировано обоснование для разработки собственного программного продукта в
рамках курсовой работы.

== Описание систем непрерывной интеграции (CI)

Непрерывная интеграция (Continuous Integration, CI) представляет собой практику
разработки программного обеспечения, при которой разработчики регулярно вносят
изменения в код основного репозитория.

Основной целью практики CI является повышение качества программного продукта и
ускорение цикла разработки за счет автоматизации процессов сборки и
тестирования. Ключевым событием, запускающим процесс, как правило, являлась
отправка (push) нового кода в систему контроля версий.

После получения уведомления о событии CI-сервер выполнял последовательность
заранее определенных шагов, именуемую пайплайн (pipeline). Стандартный конвейер
включает в себя следующие этапы:
1. Клонирование исходного кода с системы контроля версий.
2. Установка необходимых зависимостей, библиотек и инструментов.
3. Статический анализ кода.
4. Автоматический запуск тестов.
5. Сохранение исполняемых файлов, отчетов, Docker-образов.
6. По результатам выполнения пайплайна разработчикам отправляется отчет о статусе
  сборки с доступом к логам выполнения.

== Сравнительный анализ аналогов

Для проведения сравнительного анализа были выбраны три ключевые системы, каждая
из которых различный архитектурный подход в области непрерывной интеграции.

В качестве первого аналога был выбран Jenkins. Он представляет классический
подход с архитектурой "master-agent, сильной расширяемостью за счет системы
плагинов и конфигурации пайплайна через код.

Вторым объектом для анализа стала система GitLab CI. Она была выбрана как
представитель подхода интеграции CI/CD-инструментов в единую DevOps-платформу.
GitLab CI не является отдельным продуктом, а представляет собой неотъемлемую
часть GitLab, что обеспечивает взаимодействие с репозиториями, реестром
контейнеров и другими сервисами платформы.

В качестве третьего аналога была выбрана платформа GitHub Actions. Его ключевой
особенностью является концепция переиспользуемых "действий" (actions), которые
могут быть созданы сообществом и легко встроены в любой пайплайн, что
значительно ускоряет процесс его создания.

Для обеспечения объективности и структурированности анализа был определен набор
из пяти ключевых критериев. Данные критерии были выбраны с целью охватить как
технические аспекты реализации и архитектуры, так и аспекты, связанные с
пользовательским опытом и процессом эксплуатации систем.

+ Критерий "Модель конфигурации" определяет способ описания пайплайна. Анализ
  проводился на предмет того, используется ли декларативный подход (описание
  конечного состояния в формате YAML), императивный (написание скрипта с
  последовательностью действий, например, на Groovy) или конфигурация через
  графический пользовательский интерфейс (UI).

+ Критерий "Архитектура и модель развертывания" описывает внутреннее устройство
  системы и способы ее установки. Рассматривались такие архитектурные модели, как "master-agent"
  или интегрированная в платформу, а также модели развертывания: полностью
  самоуправляемая (self-hosted) или предоставляемая как облачный сервис (SaaS).

+ Среда выполнения задач. Сравнивалось окружение, в котором выполняются
  пользовательские команды. Сравнение проводилось по тому, используются ли для
  изоляции сборок Docker-контейнеры, полноценные виртуальные машины или выполнение
  происходит в операционной системе сервера-исполнителя ("агента").

+ Критерий "Уровень интеграции с Git-провайдером" оценивает, насколько тесно
  CI-система связана с конкретной платформой для хостинга репозиториев. Системы
  были разделены на универсальные, способные работать с любым Git-репозиторием, и
  платформо-зависимые, функционирующие только в рамках экосистемы.

+ Критерий "Порог вхождения и сложность настройки" отражает совокупность усилий,
  необходимых для первоначального развертывания, базовой настройки и последующей
  поддержки системы. Оценивалась как сложность написания пайплайна, так и
  сложность администрирования системы.

Был выполнен последовательный анализ каждой из трех выбранных систем в
соответствии с ранее определенными критериями.

+ Jenkins

  Jenkins представляет собой сервер автоматизации с открытым исходным кодом,
  написанный на Java. Он являлся одним из первых инструментов в своей области,
  благодаря чему получил широкое распространение и огромное сообщество.

  Jenkins поддерживает два подхода к конфигурации. Изначально создание и настройка
  задач происходила через веб-интерфейс. Однако сейчас все чаще используется
  описание конфигурации в файле Jenkinsfile с использованием императивного или
  декларативного синтаксиса на языке Groovy.

  Система построена на классической архитектуре "master-agent". Центральный узел
  (master) отвечает за хранение конфигураций, управление задачами и распределение
  нагрузки. Выполнение сборок осуществляется на подключенных к нему
  узлах-исполнителях (agents). Jenkins является self-hosted решением, то есть он
  настраивается и располагается полностью на физических устройствах пользователя.

  Задачи могут выполняться как в операционной системе узла-агента, так и в
  изолированных Docker-контейнерах.

  Система является полностью универсальной и не зависит от конкретного
  Git-провайдера. Интеграция с любыми сервисами, такими как GitHub, GitLab или
  Bitbucket, реализуется посредством установки и настройки соответствующих
  плагинов.

  Порог вхождения для работы с Jenkins оценивается как высокий. Первоначальное
  развертывание и настройка требуют значительных усилий, включая установку Java,
  управление самим сервером и конфигурацию узлов-агентов. Управление большим
  количеством плагинов также приводит к сложностям с зависимостями и обновлениями.

+ GitLab CI

  GitLab CI представляет собой компонент, интегрированный в DevOps-платформу
  GitLab.

  Для описания пайплайнов используется декларативный подход. Вся конфигурация
  задается в едином файле .gitlab-ci.yml, который располагается в корне
  репозитория и использует стандартный синтаксис YAML.

  Архитектура связана с платформой GitLab, которая выполняет роль управляющего
  узла. За выполнение задач отвечают агенты --- GitLab Runners, которые
  разворачиваются на серверах. GitLab CI доступен как в облачной версии (SaaS) на
  GitLab.com, так и как система, разворачиваемая и используемая на физических
  устройствах пользователя.

  Основной средой выполнения являются Docker-контейнеры. Также поддерживаются и
  другие среды, например, для запуска команд в Kubernetes или в оболочке сервера.

  Система полностью интегрирована и предназначена для работы с репозиториями,
  размещенными на платформе GitLab.

  Для пользователей, уже работающих с GitLab, порог вхождения является средним,
  так как CI-компонент интегрирован в систему. Процесс написания .gitlab-ci.yml
  интуитивно понятен и хорошо описан в документации. Установка и настройка
  собственного GitLab Runner требует определенных начальных усилий, однако это не
  является обязательным действием, так как Gitlab предоставляет бесплатных
  агентов.

+ GitHub Actions

  Инструмент для автоматизации, встроенный в платформу GitHub.

  Используется декларативная модель на основе YAML-файлов, которые должны
  располагаться в директории .github/workflows/. Платформа позволяет создавать
  сложные пайплайны с параллельным выполнением и зависимостями между задачами.

  Система интегрирована в GitHub, который выступает в роли оркестратора.
  Выполнение задач происходит на исполнителях (runners). GitHub предоставляет
  собственные облачные исполнители с различными операционными системами (Linux,
  Windows, macOS), а также позволяет подключать self-hosted исполнители.

  В облачной версии задачи выполняются на изолированных виртуальных машинах.
  Ключевой концепцией является "действие" (action), которое часто представляет
  собой Docker-контейнер или набор JavaScript-команд, выполняющих определенную
  логику.

  GitHub Actions предназначен для работы только с GitHub репозиториями и тесно
  интегрирован с экосистемой событий Github (push, pull request, release и т.д.).

  Порог вхождения оценивается как низкий. Начать работу можно за несколько минут
  благодаря обширному магазину готовых "действий" и качественной интеграции с
  интерфейсом GitHub. Настройка самоуправляемых исполнителей проста и
  задокументированна.

Для наглядного представления результатов проведенного анализа все данные были
сведены в @fig-ci-comparison-table[таблицу].

#figure(
  table(
    columns: (1.5fr, 1fr, 1fr, 1fr, 1fr),
    align: (left, center, center, center, center),
    [Критерий],
    [Jenkins],
    [GitLab CI],
    [GitHub Actions],
    [Swompi-Runner],
    [Модель конфигурации],
    [Groovy, UI],
    [YAML],
    [YAML],
    [YAML],
    [Архитектура и развертывание],
    [Master-Agent, Self-hosted],
    [Интегрировано, Self-hosted / SaaS],
    [Интегрировано, SaaS],
    [Единый сервис, Self-hosted],
    [Среда выполнения задач],
    [ОС агента, Docker],
    [Docker, VM],
    [VM, Docker],
    [Docker],
    [Интеграция с Git],
    [Любой Git-провайдер],
    [Только GitLab],
    [Только GitHub],
    [Любой Git-провайдер],
    [Порог вхождения],
    [Высокий],
    [Средний],
    [Низкий],
    [Низкий],
  ),
  caption: "Сравнительные характеристики систем непрерывной интеграции.",
) <fig-ci-comparison-table>

В @fig-ci-comparison-table[таблицу] также был включен разрабатываемый проект "Swompi-Runner"
для демонстрации его позиционирования относительно рассмотренных аналогов.

== Обоснование необходимости разработки собственного решения

Несмотря на широкую функциональность проанализированных систем, был выявлен ряд
факторов, которые обосновали целесообразность разработки собственного решения в
рамках курсовой работы.

Во-первых, промышленные решения обладали избыточной сложностью для небольших
проектов и требовали значительных ресурсов для развертывания и поддержки.
Интегрированные платформы, в свою очередь, привязывали разработчика к
конкретному хостингу репозиториев (GitHub или GitLab).

Во-вторых, образовательная цель проекта заключалась в изучении механизмов работы
CI-систем. Создание собственного сервера позволило на практике исследовать такие
процессы, как обработка веб-хуков, управление Docker-контейнерами через API,
захват и потоковая передача логов, а также проектирование базы данных и
хранилища файлов.

Таким образом, система "Swompi-Runner" должна быть легковесной, простой в
развертывании и поддержании инфраструктуры.

#pagebreak(weak: true)

= Выбор стека технологий

При выборе стека технологий для каждого компонента системы были рассмотрены
альтернативы, и итоговое решение принималось на основе требований к надежности и
простоте эксплуатации.

+ Язык программирования

  В качестве основного языка программирования был выбран Python 3.13.
  Рассматривался язык Go, который часто применяется для создания
  DevOps-инструментов из-за своей высокой производительности и компиляции в
  бинарный файл. Однако для данного проекта скорость разработки и обширность
  библиотек были признаны более приоритетными. Python позволил ускорить процесс
  реализации за счет наличия готовых библиотек для решения всех поставленных
  задач: от создания веб-сервера до взаимодействия с Docker API и S3-хранилищем.

+ Серверная часть

  Для реализации компонента, ответственного за прием веб-хуков от GitHub,
  рассматривались два популярных фреймворка FastAPI и Flask. Задачей сервера
  являлась реализация всего одного API-эндпоинта, поэтому функциональность FastAPI
  была признана избыточной. В связи с этим был сделан выбор в пользу
  микрофреймворка Flask.

+ База данных и файловое хранилище

  На этапе проектирования было принято архитектурное решение о разделении данных
  на два типа: структурированные метаданные (информация о сборках и репозиториях)
  и неструктурированные объектные данные (логи и артефакты).

  Для хранения структурированных данных рассматривались СУБД SQLite и PostgreSQL.
  SQLite была отклонена из-за ограничений в работе с параллельными операциями
  записи. Выбор был сделан в пользу PostgreSQL как полнофункциональной, надежной и
  производительной СУБД, легко настраеваемой через Docker-контейнер. Для
  взаимодействия с ней была выбрана ORM SQLAlchemy, которая позволила работать с
  данными в объектно-ориентированном стиле. В качестве драйвера был использован
  psycopg2-binary.

  Для хранения логов и артефактов рассматривался вариант использования локальной
  файловой системы, однако он был отклонен из-за сложностей с масштабированием и
  управлением. Далее выбор пал на MinIO, но из-за недавнего изменения политики и
  закрытия разработки open-source версии, что значит технология не будет
  развиваться дальше, было выбрано S3-совместимое объектное хранилище Garage.

+ Контейнеризация и оркестрация

  Docker был выбран как отраслевой стандарт, позволяющий запускать
  пользовательские сценарии в полностью изолированном окружении. Для программного
  управления контейнерами из Python-кода была использована библиотека docker. Для
  объединения контейнеров в единую систему и их последующее администрирование был
  выбран инструмент Docker Compose.

+ Конфигурация приложения и парсинг

  Для управления конфигурацией самого приложения была выбрана библиотека
  Pydantic-settings за ее способность автоматически загружать параметры из
  переменных окружения с одновременной валидацией типов. Для парсинга
  пользовательских файлов ".swompi.yml" была использована связка из PyYAML для
  чтения YAML-структуры и Schema для валидации. Для взаимодействия с
  Git-репозиториями была выбрана библиотека GitPython, так как она предоставляет
  более надежный и объектно-ориентированный интерфейс по сравнению с прямым
  вызовом системных команд git.

+ Реализация клиентских интерфейсов

  Для создания утилиты командной строки рассматривались стандартный модуль
  argparse и библиотека Click. Выбор был сделан в пользу Click из-за его
  современного подхода с использованием декораторов, что делает код более чистым и
  читаемым.

#pagebreak(weak: true)

= Архитектура приложения

+ Обработчик веб-хуков (файл main.py)

  Данный модуль является основной точкой входа в систему. Он был реализован как
  веб-сервер на базе Flask, прослушивающий POST-запросы на порту 25851. Модуль
  проверяет, добавлена ли ссылка на репозиторий в базу данных, если да, то
  извлекает из тела запроса информацию (URL репозитория, хэш коммита, данные об
  авторе), создает предварительную запись о новой сборке в базе данных со статусом "pending",
  запускает модуль-исполнитель и отправляет ответ со статусом 200. Если же URL
  репозитория не добавлен в систему "Swompi-Runner", то запрос отклоняется со
  статусом 400. Запуск веб-сервера показан на @image2[рисунке].

  #figure(image("img/image copy.png"), caption: [Запуск веб-сервера]) <image2>

+ Исполнитель (файл executor.py)

  Реализован класс Executor, который при инициализации запрашивает фабрику
  создания сессий к базе данных, объект клиента файлового хранилища, объект Config
  для доступа к переменным окружения. Пример логов исполнителя показан на @image3

  #figure(image("img/image copy 2.png"), caption: [Логи исполнителя]) <image3>

  Функции класса:
  - Основной управляющий метод run_build. Данный метод является основной точкой
    входа в класс и управляет всем жизненным циклом выполнения пайплайна. Его работа
    обернута в конструкцию try...except...finally для гарантированной обработки
    ошибок и очистки ресурсов. В рамках его выполнения последовательно вызывались
    остальные приватные методы.

  - Подготовка временной директории (\_prepare_workspace). Для обеспечения полной
    изоляции каждой сборки была реализована функция, которая с использованием
    стандартной библиотеки tempfile создавала уникальную временную директорию в
    системной папке /tmp/. Для удобства отладки каждой директории был присвоен
    префикс swompi_build_\{build_id\}_.

  - Клонирование репозитория (\_clone_repo). На следующем этапе была реализована
    логика клонирования Git-репозитория. С помощью библиотеки GitPython выполнялась
    команда clone_from, загружавшая исходный код, и выполнялся переход (checkout) к
    конкретному коммиту, инициировавшему запуск пайплайна.

  - Чтение и валидация конфигурационного файла (\_read_and_validate_config). Сначала
    вспомогательный метод \_find_config_file находил файл ".swompi.yml" в корне
    репозитория. Затем основной метод \_read_and_validate_config валидировал его
    содержимое. С использованием библиотеки Schema была определена строгая структура
    ожидаемого файла, а чтение и парсинг YAML-структуры осуществлялись с помощью
    PyYAML.

  - Формирование переменных окружения (\_create_enviroment_dict). Был реализован
    метод, который формировал словарь переменных окружения для передачи в
    Docker-контейнер. Этот словарь включал как предопределенные системой переменные
    (CI_COMMIT_SHA, CI_COMMIT_MESSAGE, CI_COMMIT_AUTHOR, CI_PROJECT_DIR,
    CI_REPO_URL, CI_BUILD_ID, CI_SERVER_NAME, CI_COMMIT_REF_NAME), так и
    пользовательские, указанные в файле ".swompi.yml".

  - Создание исполняемого скрипта (\_create_build_script). Был применен подход с
    созданием временного shell-скрипта \_run.sh. В этот скрипт последовательно
    записывались команды из секций before_script и scripts с добавлением директивы
    set -e, гарантирующей завершение работы при возникновении любой ошибки.

  - Управление Docker-контейнером (\_run_docker_container). С помощью библиотеки
    docker был реализован алгоритм, включавший загрузку Docker-образа, создание и
    запуск контейнера с монтированной временной директорией и переменными окружения,
    а также захват логов в реальном времени.

  - Обработка сбоев (\_mark_build_as_failed). В случае возникновения исключения или
    при получении ненулевого кода выхода от контейнера вызывался данный метод. Он
    обновлял статус текущей сборки в базе данных на failed.

  - Очистка рабочего пространства (\_cleanup_workspace). Был реализован метод,
    который вызывался в блоке finally основного метода. Он полностью и рекурсивно
    удалял временную директорию со всем ее содержимым.

+ Взаимодействие с базой данных

+ Файловое хранилище (файл storage.py)

  Для инкапсуляции всей логики взаимодействия с S3-совместимым файловым хранилищем
  Garage был разработан класс FileStorageRepository. Данный компонент отвечал за
  загрузку и скачивание файлов, а также за первоначальную настройку окружения в
  хранилище. Его функциональность была разделена на следующие методы:

  - Инициализация и подключение к S3 (\_create_s_client). При создании экземпляра
    класса в конструктор \__init_\_ передавался объект конфигурации приложения. Этот
    метод использовал данные для инициализации клиента boto3. Для обеспечения
    совместимости с self-hosted решением Garage была явно указана версия подписи
    s3v4 и применен path-style для адресации бакетов.

  - Автоматическое создание бакета (\_ensure_buckets_exist). Метод, вызывающийся
    сразу после инициализации клиента, реализовывал принцип идемпотентности: он
    отправлял запрос head_bucket для проверки существования необходимого бакета "swompi-runner".
    В случае, если бакет не был найден, метод автоматически отправлял команду на его
    создание.

  - Загрузка логов и артефактов (upload_logs_and_artifacts). Все файлы сборки,
    включая лог и артефакты, объединяются в один архив. Для этого создается
    временная директория, в которую перемещаются файл build.log и все файлы,
    указанные в секции artifacts. Затем, с использованием библиотеки py7zr,
    содержимое директории сжимается в единый архив формата .7z, который загружается
    в S3-хранилище под именем, соответствующим идентификатору сборки. Метод
    возвращает имя созданного объекта.

  - Скачивание файлов (download_file_to_path). Для выгрузки архива метод принимает
    на вход ключ объекта и локальный путь для сохранения, после чего выполняется
    скачивание файла.

+ Модуль конфигурации (файл config.py)

  Для управления конфигурационными параметрами приложения, такими как учетные
  данные для доступа к базам данных и внешним API, был применен подход, основанный
  на отделении конфигурации от кода.

  Вся конфигурация была инкапсулирована в классе AppConfig, унаследованном от
  класса BaseSettings. Внутри этого класса были объявлены все необходимые для
  работы системы параметры в виде атрибутов с явным указанием типов данных.

  Для параметров POSTGRES_HOSTNAME, S3_ENDPOINT_URL, S3_DEFAULT_REGION были заданы
  значения по умолчанию. Для переменных S3_ACCESS_KEY, S3_SECRET_KEY и
  TELEGRAM_BOT_TOKEN, значения по умолчанию отсутствовали. Приложение не может
  запуститься без предоставления этих параметров, что исключает ошибки во время
  выполнения, связанные с отсутствием необходимых учетных данных.

  Переменные POSTGRES_DB и POSTGRES_USER указываются в файле .env в корневой папке
  приложения "Swompi-Runner".

  Если же переменные не были переданы, то выходит ошибка, показанная на
  @image4[Рисунке].

  #figure(
    image("img/image copy 3.png"),
    caption: [Ошибка при отсутствии переменных окружения],
  ) <image4>

+ Интерфес командной строки (файл cli.py)

  Для управления системой был разработан интерфейс командной строки для работы со
  списком отслеживаемых репозиториев. Для создания CLI была выбрана библиотека
  Click.

  Основная логика была сгруппирована вокруг объекта group, который соответствовал
  главной исполняемой команде swompi. К этой группе были привязаны три подкоманды,
  реализующие базовые CRUD-операции.

  - Команда ls. Была предназначена для отображения полного списка репозиториев,
    которые в данный момент отслеживаются системой.

  - Команда create. Позволяет добавлять новые репозитории для отслеживания. Была
    добавлена проверка входных данных, гарантирующая, что предоставленный URL
    действительно является ссылкой на сервис GitHub.

  - Команда delete. Используется для удаления репозитория из списка отслеживаемых.
    Метод получал URL репозитория в качестве единственного аргумента, после чего
    вызывал функцию базы данных, которая выполняла поиск и удаление соответствующей
    записи.

  Пример использования интерфейса командной строки показан на @image4[Рисунке].

  #figure(
    image("img/image copy 4.png"),
    caption: [Использование командной строки],
  ) <image5>

+ Telegram--бот (файл bot.py)

Структура проекта показана на @image1[рисунке].

#figure(image("img/image.png"), caption: [Структура проекта]) <image1>

#pagebreak(weak: true)

= Развертывание с помощью Docker

Для развертывания системы "Swompi-Runner" был выбран подход, основанный на
контейнеризации всех ее компонентов с помощью Docker и их оркестрации
посредством Docker Compose.

== Настройка окружения с помощью Docker Compose

Вся конфигурация окружения была описана в файле docker-compose.yml. Были
определены три основных сервиса: swompi-runner (основное приложение), database
(сервер СУБД PostgreSQL) и garage (S3-совместимое файловое хранилище).

Для обеспечения персистентности данных были использованы именованные тома
(volumes): database для хранения данных PostgreSQL, garage_storage и
garage_config для данных и конфигурации Garage, а также shared_data. Последний
том используется как общее пространство для обмена конфигурационными файлами
между сервисами. Так как у garage нет возможности задать пользовательские ключи
доступа, то при первом запуске генерируется файл с учетными данными для доступа
к S3, через shared_data сервис swompi-runner считывает его при своем старте.

Управление конфигурацией было реализовано через файлы переменных окружения ---
.env файлы. Для сервиса database можно настроить учетные данные пользователя
PostgreSQL, а для swompi-runner --- получение необходимых токенов и секретных
ключей (S3_ENDPOINT_URL, S3_ACCESS_KEY, S3_SECRET_KEY, TELEGRAM_BOT_TOKEN).

Корректный порядок запуска сервисов был обеспечен с помощью директивы depends_on
в конфигурации Docker Compose. Было определено, что сервис swompi-runner должен
запускаться только после сервисов database и garage. Для точного определения
момента готовности последних были настроены проверки состояния (healthcheck),
которые выполняли специфичные для каждого сервиса команды для валидации их
работоспособности.

== Сборка образа сервиса "Swompi-Runner"

Для основного приложения был разработан собственный образ на основе официального
python:3.13-slim-bookworm. Процесс сборки, описанный в файле Dockerfile, был
разделен на несколько логических этапов.

+ На первом этапе в образ были установлены зависимости для библиотек GitPython и
  docker --- git и docker-ce-cli соответственно. Полная установка Docker Engine в
  контейнере не требуется, так как приложение будет взаимодействовать с
  Docker-демоном хост-машины для запуска контейнеров сборок. Это взаимодействие
  было реализовано путем монтирования сокета Docker в контейнер.
+ Сначала в образ копировался только файл pyproject.toml, и выполнялась команда
  poetry install, что создает отдельный слой в образе, содержащий все зависимости.
  На следующем шаге копировался остальной исходный код. Такой подход использует
  механизм кеширования Docker: при повторном сборе образа, если были изменения в
  исходном коде, переустановка всех зависимостей не требуется, что значительно
  ускоряет процесс сборки.
+ В завершение в образ копировался скрипт entrypoint.sh, который был назначен
  точкой входа контейнера.

== Сборка образа сервиса "Garage"

В ходе работы была выявлена невозможность использования официального образа
Garage для автоматической инициализации сервиса. В связи с этим был разработан и
опубликован#footnote[URL: https://hub.docker.com/r/orlovchik/garage] модифицированный
образ, созданный с помощью многоэтапной сборки. Для обеспечения универсальности
образ поддерживает две ключевые архитектуры: amd64 и arm64, что позволяет
разворачивать систему на различном серверном оборудовании.

#figure(image("img/image copy 5.png"), caption: [Образ orlovchik/garage]) <image6>

Этапы сборки образа:

+ На первом этапе использовался легковесный образ "alpine/openssl" для выполнения
  скрипта init_config.sh. Этот скрипт генерировал конфигурационный файл
  garage.toml с динамически созданными секретными ключами.
+ В качестве основы для финального образа был выбран "debian:11-slim", так как в
  нем присутствует bash, нужный для выполнения сценария. В него копировался
  бинарный файл сервера Garage, а также скрипт инициализации "init_garage.sh".
  Ключевым моментом являлось копирование файла garage.toml, сгенерированного на
  предыдущем этапе, из сборочного контейнера в финальный образ. Такой подход
  позволил сохранить финальный образ легковесным, не включая в него утилиту
  openssl, которая нужна была только для генерации ключей хранилища.

Код "Dockerfile" для хранилища garage показан на @image8[Рисунке].

#figure(
  image("img/image copy 7.png"),
  caption: [Содержимое файла garage/Dockerfile],
) <image8>

== Автоматическая инициализация сервисов

Ключевая логика по первоначальной настройке системы была вынесена в
entrypoint-скрипты для сервисов garage и swompi-runner.

+ Скрипт "init_garage.sh" при первом запуске временно запускал сервер Garage в
  фоновом режиме, выполнял необходимые команды для инициализации кластера и
  компоновки, а после чего создавал S3-ключ доступа для приложения Swompi.
  Полученные ACCESS_KEY и SECRET_KEY записывались в файл ".garage.env" в общем
  томе shared_data. После успешного завершения инициализации фоновый процесс
  Garage останавливался, и запускался основной сервер в штатном режиме. При
  последующих запусках скрипт обнаруживал уже созданный ".garage.env" и немедленно
  запускал сервер.

+ Скрипт "entrypoint.sh" для основного приложения также содержал логику ожидания и
  настройки. В его начале была добавлена задержка для гарантированного запуска
  зависимых сервисов. Основной задачей скрипта было дождаться появления файла ".garage.env"
  в общем томе и загрузить из него переменные окружения в текущую сессию с помощью
  команды source. После этого управление передавалось основному процессу Python,
  который запускал файл "main.py".

= Структура ".swompi.yml" файла

Для управления процессом сборки и тестирования был выбран подход "Configuration
as Code". Вся логика пайплайна была вынесена в единый конфигурационный файл с
именем ".swompi.yml", который должен располагаться в корне репозитория. В
качестве формата был выбран YAML из-за высокой читаемости.

+ image. Обязательная директива, определяющая Docker-образ, в окружении которого
  будут выполняться все последующие команды. Значение этого поля используется
  модулем-исполнителем для загрузки образа из Docker Hub.

+ variables. Опциональная секция, которая позволяет определять переменные
  окружения в формате "ключ-значение". Эти переменные становятся доступными на
  всех этапах выполнения пайплайна внутри Docker-контейнера.

+ before_script. Опциональная секция, предназначенная для выполнения
  подготовительных команд, таких как установка системных зависимостей или загрузка
  пакетов.

+ scripts. Обязательная секция. Содержит список команд. Команды выполняются
  последовательно. В случае, если любая из команд заевршается с ненулевым кодом
  выхода, весь пайплайн останавливается и помечается как проваленный.

+ after_script. Опциональная секция, предназначенная для выполнения завершающих
  операций. Команды из этого блока гарантированно выполняются после секции
  scripts, независимо от результата ее выполнения.

+ artifacts. Опциональная секция, которая предназначается для сохранения файлов,
  сгенерированных в процессе выполнения пайплайна. Внутри нее с помощью ключа
  paths указывается список файлов, которые необходимо было сохранить. После
  успешного завершения сборки указанные файлы архивируются и загружаются в
  файловое хранилище.

Пример конфигурационного файла ".swompi.yml" представлен на @image7[Рисунке].

#figure(image("img/image copy 6.png"), caption: [Файл ".swompi.yml"]) <image7>

= Демонстрация работы

Первым действием было выполнено клонирование исходного кода проекта из
Git-репозитория на целевой сервер. Сразу после этого была инициирована сборка и
запуск всех компонентов системы с помощью единой команды "docker-compose up
--build -d". Данная команда автоматически выполнила сборку кастомных образов для
сервисов "swompi-runner" и "garage", после чего запустила всю необходимую
инфраструктуру, включая базу данных PostgreSQL. Успешный запуск всех сервисов со
статусом "healthy" представлен на @fig-compose-up[рисунке].

#figure(
  image("img/image copy 8.png"),
  caption: "Результат успешного запуска всех сервисов системы с помощью Docker Compose.",
) <fig-compose-up>

После успешного развертывания была выполнена настройка системы от лица
администратора. С помощью утилиты командной строки, запущенной внутри
Docker-контейнера "swompi-runner", была выполнена команда "swompi create" для
добавления нового репозитория в список отслеживаемых. В качестве аргументов были
переданы HTTPS URL репозитория и его короткое имя. Результат выполнения команды
представлен на @fig-cli-add-repo[рисунке].

#figure(
  image("img/image copy 9.png"),
  caption: "Результат выполнения команды добавления нового репозитория.",
) <fig-cli-add-repo>

Был настроен веб-хук в репозитории на GitHub. В настройках веб-хука был указан
публичный IP-адрес сервера и порт, на котором сервис "swompi-runner" принимал
входящие подключения. В качестве типа контента был выбран "application/json", а
в качестве события, инициирующего отправку уведомления, --- событие "push".
Настройки веб-хука показаны на @fig-webhook-setup[рисунке].

#figure(
  image("img/image copy 10.png"),
  caption: "Конфигурация веб-хука в настройках репозитория GitHub.",
) <fig-webhook-setup>

Для запуска пайплайна была выполнена стандартная операция разработчика ---
отправка нового коммита в отслеживаемую ветку репозитория. Сразу после получения
веб-хука от GitHub система автоматически запустила процесс выполнения пайплайна.
По его завершении было сформировано и отправлено итоговое уведомление в
Telegram, информирующее об успешном завершении работы. Пример уведомления можно
увидеть на @fig-tg-notification[рисунке].

#figure(
  image("img/image copy 11.png"),
  caption: "Уведомление об успешном завершении сборки в Telegram.",
) <fig-tg-notification>

Для получения детальной информации о ходе выполнения сборки был использован
Telegram-бот. Пользователем были последовательно отправлены команды "/status" и "/artifacts"
с указанием идентификатора сборки. В ответ бот предоставил их пользователю в 7z
архива. Данное взаимодействие продемонстрировано на @fig-tg-results[рисунке].

#figure(
  image("img/image copy 12.png"),
  caption: "Получение файлов с логами и артефактами сборки через Telegram-бота.",
) <fig-tg-results>

Таким образом, был успешно продемонстрирован полный жизненный цикл работы
системы.

#gost.struct-heading("ЗАКЛЮЧЕНИЕ")

#gost.struct-heading("СПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ")